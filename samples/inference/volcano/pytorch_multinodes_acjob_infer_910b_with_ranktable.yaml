apiVersion: mindxdl.gitee.com/v1
kind: AscendJob
metadata:
  name: default-infer-test-pytorch-910b
  labels:
    framework: pytorch
    ring-controller.atlas: ascend-910b
    fault-scheduling: "force"
spec:
  schedulerName: volcano   # work when enableGangScheduling is true
  runPolicy:
    schedulingPolicy:      # work when enableGangScheduling is true
      minAvailable: 2
      queue: default
  successPolicy: AllWorkers
  replicaSpecs:
    Master:
      replicas: 1
      restartPolicy: Never
      template:
        metadata:
          labels:
            ring-controller.atlas: ascend-910b
        spec:
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchExpressions:
                      - key: job-name
                        operator: In
                        values:
                          - default-infer-test-pytorch-910b
                  topologyKey: kubernetes.io/hostname
          nodeSelector:
            host-arch: huawei-arm
            accelerator-type: module-910b-8 # depend on your device model, 910bx8 is module-910b-8 ,910bx16 is module-910b-16
          containers:
            - name: ascend # do not modify
              image: pytorch-test:latest         # trainning framework image， which can be modified
              imagePullPolicy: IfNotPresent
              env:
                - name: XDL_IP                                       # IP address of the physical node, which is used to identify the node where the pod is running
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                # ASCEND_VISIBLE_DEVICES env variable is used by ascend-docker-runtime when in the whole card scheduling scene with volcano scheduler. please delete it when in the static vNPU scheduling scene or without volcano.
                - name: ASCEND_VISIBLE_DEVICES
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.annotations['huawei.com/Ascend910']               # The value must be the same as resources.requests
              command:                           # training command, which can be modified
                - /bin/bash
                - -c
              args: [ "./infer.sh" ]
              ports:                          # default value containerPort: 2222 name: ascendjob-port if not set
                - containerPort: 2222         # determined by user
                  name: ascendjob-port        # do not modify
              resources:
                limits:
                  huawei.com/Ascend910: 2
                requests:
                  huawei.com/Ascend910: 2
              volumeMounts:
                - name: ascend-driver
                  mountPath: /usr/local/Ascend/driver
                - name: ascend-add-ons
                  mountPath: /usr/local/Ascend/add-ons
                - name: dshm
                  mountPath: /dev/shm
                - name: ranktable                            # if ranktable is needed please set volume
                  mountPath: /user/serverid/devindex/config
                - name: localtime
                  mountPath: /etc/localtime
          volumes:
            - name: ascend-driver
              hostPath:
                path: /usr/local/Ascend/driver
            - name: ascend-add-ons
              hostPath:
                path: /usr/local/Ascend/add-ons
            - name: dshm
              emptyDir:
                medium: Memory
            - name: ranktable
              hostPath:
                path: /user/mindx-dl/ranktable/default.default-infer-test-pytorch-910b  # need pattern: shared-dir/job-namespace.job-name, the shared-dir should share with ascend-operator
            - name: localtime
              hostPath:
                path: /etc/localtime
    Worker:
      replicas: 1
      restartPolicy: Never
      template:
        metadata:
          labels:
            ring-controller.atlas: ascend-910b
        spec:
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchExpressions:
                      - key: job-name
                        operator: In
                        values:
                          - default-infer-test-pytorch-910b
                  topologyKey: kubernetes.io/hostname
          nodeSelector:
            host-arch: huawei-arm
            accelerator-type: module-910b-8 # depend on your device model, 910bx8 is module-910b-8 ,910bx16 is module-910b-16
          containers:
            - name: ascend # do not modify
              image: pytorch-test:latest                # trainning framework image， which can be modified
              imagePullPolicy: IfNotPresent
              env:
                - name: XDL_IP                                       # IP address of the physical node, which is used to identify the node where the pod is running
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                # ASCEND_VISIBLE_DEVICES env variable is used by ascend-docker-runtime when in the whole card scheduling scene with volcano scheduler. please delete it when in the static vNPU scheduling scene or without volcano.
                - name: ASCEND_VISIBLE_DEVICES
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.annotations['huawei.com/Ascend910']               # The value must be the same as resources.requests
              command:                                  # training command, which can be modified
                - /bin/bash
                - -c
              args: [ "./infer.sh" ]
              ports:                          # default value containerPort: 2222 name: ascendjob-port if not set
                - containerPort: 2222         # determined by user
                  name: ascendjob-port        # do not modify
              resources:
                limits:
                  huawei.com/Ascend910: 2
                requests:
                  huawei.com/Ascend910: 2
              volumeMounts:
                - name: ascend-driver
                  mountPath: /usr/local/Ascend/driver
                - name: ascend-add-ons
                  mountPath: /usr/local/Ascend/add-ons
                - name: dshm
                  mountPath: /dev/shm
                - name: ranktable                           # if ranktable is needed please set volume
                  mountPath: /user/serverid/devindex/config
                - name: localtime
                  mountPath: /etc/localtime
          volumes:
            - name: ascend-driver
              hostPath:
                path: /usr/local/Ascend/driver
            - name: ascend-add-ons
              hostPath:
                path: /usr/local/Ascend/add-ons
            - name: dshm
              emptyDir:
                medium: Memory
            - name: ranktable
              hostPath:
                path: /user/mindx-dl/ranktable/default.default-infer-test-pytorch-910b # need pattern: shared-dir/job-namespace.job-name, the shared-dir should share with ascend-operator
            - name: localtime
              hostPath:
                path: /etc/localtime


