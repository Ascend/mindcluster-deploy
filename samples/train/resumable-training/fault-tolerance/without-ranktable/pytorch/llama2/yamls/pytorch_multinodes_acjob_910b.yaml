apiVersion: v1
kind: ConfigMap
metadata:
  name: reset-config-default-test-pytorch     # The value of JobName must be the same as the name attribute of the following job. The prefix fault-config- cannot be modified.
  namespace: default                      # Name Space must be set to "default"
  labels:
    reset: "true"
data:
  reset.json: |
    {
        "status":"initializing"
    }
  checkCode: ""
---
apiVersion: mindxdl.gitee.com/v1
kind: AscendJob
metadata:
  name: default-test-pytorch
  labels:
    framework: pytorch
    ring-controller.atlas: ascend-910b
    tor-affinity: "null" #该标签为任务是否使用交换机亲和性调度标签，null或者不写该标签则不适用。large-model-schema表示大模型任务，normal-schema 普通任务
    fault-scheduling: "force"
    fault-retry-times: "10"
    pod-rescheduling: "on" # 开启pod重调度
    process-recover-enable: "on" # 开启训练进程级恢复
    subHealthyStrategy: "ignore" # 忽略亚健康
  annotations:
    wait-reschedule-timeout: "270" # 进程级恢复等待故障节点重调度的超时时间，默认270秒，取值范围30-270。进程级恢复和弹性训练均开启时，则等待此时间后若故障节点调度成功，则进行进程级恢复，否则触发弹性训练
    recover-strategy: "retry,recover,dump,exit,elastic-training" # 开启step重计算、进程级恢复、临终遗言、进程退出和弹性训练
spec:
  schedulerName: volcano   # work when enableGangScheduling is true
  runPolicy:
    schedulingPolicy:      # work when enableGangScheduling is true
      minAvailable: 2
      queue: default
  successPolicy: AllWorkers
  replicaSpecs:
    Master:
      replicas: 1
      restartPolicy: Never
      template:
        metadata:
          labels:
            ring-controller.atlas: ascend-910b
        spec:
          terminationGracePeriodSeconds: 900
          automountServiceAccountToken: false
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchExpressions:
                      - key: job-name
                        operator: In
                        values:
                          - default-test-pytorch
                  topologyKey: kubernetes.io/hostname
          nodeSelector:
            host-arch: huawei-arm
            accelerator-type: module-910b-8 # depend on your device model, 910bx8 is module-910b-8 ,910bx16 is module-910b-16
          hostNetwork: true
          containers:
          - name: ascend # do not modify
            image: pytorch-test:latest         # trainning framework image， which can be modified
            imagePullPolicy: IfNotPresent
            env:
              - name: XDL_IP                                       # IP address of the physical node, which is used to identify the node where the pod is running
                valueFrom:
                  fieldRef:
                    fieldPath: status.hostIP
              # ASCEND_VISIBLE_DEVICES env variable is used by ascend-docker-runtime when in the whole card scheduling scene with volcano scheduler. 
              # Please delete it when in the static vNPU scheduling, dynamic vNPU scheduling, volcano without Ascend-volcano-plugin, without volcano scenes.
              - name: ASCEND_VISIBLE_DEVICES
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['huawei.com/Ascend910']               # The value must be the same as resources.requests
              - name: TTP_PORT                # 用于mindio通信，请注意上下保持一致
                value: "8000"
              - name: PROCESS_RECOVER         # 开启进程级别重调度需注入该环境变量
                value: "on"
              - name: MINDIO_WAIT_MINDX_TIME         # mindio暂停训练后等待恢复策略下发时间，未开启进程级重调度，开启弹性训练场景下，建议配置60及以上
                value: "60"
              - name: POD_IP
                valueFrom:
                  fieldRef:
                    fieldPath: status.podIP
            command:                           # training command, which can be modified
              - /bin/bash
              - -c
            args:
              - | 
                cd /job/code; 
                source /usr/local/Ascend/ascend-toolkit/set_env.sh; 
                export PYTHONPATH=/job/code/MindSpeed:$PYTHONPATH; 
                export LOGLEVEL=DEBUG; 
                export ELASTIC_PROCESS_RECOVER_ENABLE=1;
                chmod +x scripts/train_start.sh; 
                bash scripts/train_start.sh /job/code /job/output pretrain_gpt.py \
                  --tensor-model-parallel-size 8 \
                  --pipeline-model-parallel-size 1 \
                  --sequence-parallel --num-layers 32 \
                  --hidden-size 4096 \
                  --ffn-hidden-size 11008 \
                  --num-attention-heads 32 \
                  --tokenizer-type Llama2Tokenizer \
                  --seq-length 4096 \
                  --max-position-embeddings 4096 \
                  --micro-batch-size 1 \
                  --global-batch-size 256 \
                  --make-vocab-size-divisible-by 1 \
                  --lr 1.25e-6 \
                  --train-iters 5000 \
                  --lr-decay-style cosine \
                  --untie-embeddings-and-output-weights \
                  --disable-bias-linear \
                  --attention-dropout 0.0 \
                  --init-method-std 0.01 \
                  --hidden-dropout 0.0 \
                  --position-embedding-type rope \
                  --normalization RMSNorm \
                  --use-fused-rmsnorm \
                  --swiglu \
                  --use-flash-attn \
                  --no-masked-softmax-fusion \
                  --attention-softmax-in-fp32 \
                  --min-lr 1.25e-7 \
                  --weight-decay 1e-1 \
                  --lr-warmup-fraction 0.01 \
                  --clip-grad 1.0 \
                  --adam-beta1 0.9 \
                  --initial-loss-scale 65536 \
                  --adam-beta2 0.95 \
                  --no-gradient-accumulation-fusion \
                  --no-load-optim \
                  --no-load-rng \
                  --use-distributed-optimizer \
                  --use-fused-swiglu \
                  --use-fused-rotary-pos-emb \
                  --overlap-grad-reduce \
                  --bf16 \
                  --enable-high-availability \
                  --enable-hbmfault-repair \
                  --enable-worker-reboot \
                  --enable-elastic-training \
                  --data-path $DATA_PATH \
                  --split 949501 \
                  --log-interval 1 \
                  --save-interval 20 \
                  --eval-interval 1000 \
                  --eval-iters 10 \
                  --distributed-backend nccl
            # --enable-high-availability：故障快速恢复特性开关，默认关闭，配置后即开启临终遗言功能。
            # --enable-hbmfault-repair：Step级别重计算恢复功能开关，默认关闭，配置后对片上内存进行故障检测，并完成在线修复。需要同时配置--enable-high-availability。
            # --enable-worker-reboot：进程级别重调度功能开关，默认关闭，配置后在发生一般性故障时，进行进程级别调度，继续训练。需同时配置--enable-high-availability。
            # --enable-elastic-training：弹性训练功能开关源，默认关闭，配置后在发生一般性故障，且集群中无可用空闲资源时，缩容继续训练。需同时配置--enable-high-availability。
            ports:                          # default value containerPort: 2222 name: ascendjob-port if not set
              - containerPort: 2222         # determined by user
                name: ascendjob-port        # do not modify
              - containerPort: 8000         # 用于mindio通信，请注意上下保持一致
                name: ttp-port
            resources:
              limits:
                huawei.com/Ascend910: 8
              requests:
                huawei.com/Ascend910: 8
            volumeMounts:
            - name: code
              mountPath: /job/code
            - name: data
              mountPath: /job/data
            - name: output
              mountPath: /job/output
            - name: ascend-driver
              mountPath: /usr/local/Ascend/driver
            - name: dshm
              mountPath: /dev/shm
            - name: localtime
              mountPath: /etc/localtime
            - name: reset-config
              mountPath: /user/restore/reset/config
              readOnly: true
          volumes:
          - name: reset-config
            hostPath:
              path: /user/restore/reset/default.reset-config-default-test-pytorch                # Correspond to the /user/restore/reset/namespace.configmap-name above.
          - name: code
            nfs:
              server: 127.0.0.1
              path: "/data/atlas_dls/public/code/LLAMA2_for_PyTorch_2.1_code/"
          - name: data
            nfs:
              server: 127.0.0.1
              path: "/data/atlas_dls/public/dataset/"
          - name: output
            nfs:
              server: 127.0.0.1
              path: "/data/atlas_dls/output/"
          - name: ascend-driver
            hostPath:
              path: /usr/local/Ascend/driver
          - name: dshm
            emptyDir:
              medium: Memory
          - name: localtime
            hostPath:
              path: /etc/localtime
    Worker:
      replicas: 1
      restartPolicy: Never
      template:
        metadata:
          labels:
            ring-controller.atlas: ascend-910b
        spec:
          terminationGracePeriodSeconds: 900
          automountServiceAccountToken: false
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchExpressions:
                      - key: job-name
                        operator: In
                        values:
                          - default-test-pytorch
                  topologyKey: kubernetes.io/hostname
          nodeSelector:
            host-arch: huawei-arm
            accelerator-type: module-910b-8 # depend on your device model, 910bx8 is module-910b-8 ,910bx16 is module-910b-16
          hostNetwork: true
          containers:
          - name: ascend # do not modify
            image: pytorch-test:latest                # trainning framework image， which can be modified
            imagePullPolicy: IfNotPresent
            env:
              - name: XDL_IP                                       # IP address of the physical node, which is used to identify the node where the pod is running
                valueFrom:
                  fieldRef:
                    fieldPath: status.hostIP
              # ASCEND_VISIBLE_DEVICES env variable is used by ascend-docker-runtime when in the whole card scheduling scene with volcano scheduler. 
          # Please delete it when in the static vNPU scheduling, dynamic vNPU scheduling, volcano without Ascend-volcano-plugin, without volcano scenes.
              - name: ASCEND_VISIBLE_DEVICES
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['huawei.com/Ascend910']               # The value must be the same as resources.requests
              - name: TTP_PORT                # 用于mindio通信，请注意上下保持一致
                value: "8000"
              - name: PROCESS_RECOVER         # 开启进程级别重调度需注入该环境变量
                value: "on"
              - name: MINDIO_WAIT_MINDX_TIME         # mindio暂停训练后等待恢复策略下发时间，未开启进程级重调度，开启弹性训练场景下，建议配置60及以上
                value: "60"
              - name: POD_IP
                valueFrom:
                  fieldRef:
                    fieldPath: status.podIP
            command:                                  # training command, which can be modified
              - /bin/bash
              - -c
            args:
              - |
                cd /job/code;
                source /usr/local/Ascend/ascend-toolkit/set_env.sh;
                export PYTHONPATH=/job/code/MindSpeed:$PYTHONPATH;
                export LOGLEVEL=DEBUG;
                export ELASTIC_PROCESS_RECOVER_ENABLE=1;
                chmod +x scripts/train_start.sh;
                bash scripts/train_start.sh /job/code /job/output pretrain_gpt.py \
                --tensor-model-parallel-size 8 \
                --pipeline-model-parallel-size 1 \
                --sequence-parallel --num-layers 32 \
                --hidden-size 4096 \
                --ffn-hidden-size 11008 \
                --num-attention-heads 32 \
                --tokenizer-type Llama2Tokenizer \
                --seq-length 4096 \
                --max-position-embeddings 4096 \
                --micro-batch-size 1 \
                --global-batch-size 256 \
                --make-vocab-size-divisible-by 1 \
                --lr 1.25e-6 \
                --train-iters 5000 \
                --lr-decay-style cosine \
                --untie-embeddings-and-output-weights \
                --disable-bias-linear \
                --attention-dropout 0.0 \
                --init-method-std 0.01 \
                --hidden-dropout 0.0 \
                --position-embedding-type rope \
                --normalization RMSNorm \
                --use-fused-rmsnorm \
                --swiglu \
                --use-flash-attn \
                --no-masked-softmax-fusion \
                --attention-softmax-in-fp32 \
                --min-lr 1.25e-7 \
                --weight-decay 1e-1 \
                --lr-warmup-fraction 0.01 \
                --clip-grad 1.0 \
                --adam-beta1 0.9 \
                --initial-loss-scale 65536 \
                --adam-beta2 0.95 \
                --no-gradient-accumulation-fusion \
                --no-load-optim \
                --no-load-rng \
                --use-distributed-optimizer \
                --use-fused-swiglu \
                --use-fused-rotary-pos-emb \
                --overlap-grad-reduce \
                --bf16 \
                --enable-high-availability \
                --enable-hbmfault-repair \
                --enable-worker-reboot \
                --enable-elastic-training \
                --data-path $DATA_PATH \
                --split 949501 \
                --log-interval 1 \
                --save-interval 20 \
                --eval-interval 1000 \
                --eval-iters 10 \
                --distributed-backend nccl
            # --enable-high-availability：故障快速恢复特性开关，默认关闭，配置后即开启临终遗言功能。
            # --enable-hbmfault-repair：Step级别重计算恢复功能开关，默认关闭，配置后对片上内存进行故障检测，并完成在线修复。需要同时配置--enable-high-availability。
            # --enable-worker-reboot：进程级别重调度功能开关，默认关闭，配置后在发生一般性故障时，进行进程级别调度，继续训练。需同时配置--enable-high-availability。
            # --enable-elastic-training：弹性训练功能开关源，默认关闭，配置后在发生一般性故障，且集群中无可用空闲资源时，缩容继续训练。需同时配置--enable-high-availability。
            ports:                          # default value containerPort: 2222 name: ascendjob-port if not set
              - containerPort: 2222         # determined by user
                name: ascendjob-port        # do not modify
              - containerPort: 8000         # 用于mindio通信，请注意上下保持一致
                name: ttp-port
            resources:
              limits:
                huawei.com/Ascend910: 8
              requests:
                huawei.com/Ascend910: 8
            volumeMounts:
            - name: code
              mountPath: /job/code
            - name: data
              mountPath: /job/data
            - name: output
              mountPath: /job/output
            - name: ascend-driver
              mountPath: /usr/local/Ascend/driver
            - name: dshm
              mountPath: /dev/shm
            - name: localtime
              mountPath: /etc/localtime
            - name: reset-config
              mountPath: /user/restore/reset/config
              readOnly: true
          volumes:
          - name: reset-config
            hostPath:
              path: /user/restore/reset/default.reset-config-default-test-pytorch                # Correspond to the /user/restore/reset/namespace.configmap-name above.
          - name: code
            nfs:
              server: 127.0.0.1
              path: "/data/atlas_dls/public/code/LLAMA2_for_PyTorch_2.1_code/"
          - name: data
            nfs:
              server: 127.0.0.1
              path: "/data/atlas_dls/public/dataset/"
          - name: output
            nfs:
              server: 127.0.0.1
              path: "/data/atlas_dls/output/"
          - name: ascend-driver
            hostPath:
              path: /usr/local/Ascend/driver
          - name: dshm
            emptyDir:
              medium: Memory
          - name: localtime
            hostPath:
              path: /etc/localtime


